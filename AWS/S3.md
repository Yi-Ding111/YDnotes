# S3



## Use Boto3 create bucket

[s3_client.create_bucket()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/create_bucket.html)

[s3_client.put_object()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_object.html)

[s3_client.put_bucket_versioning()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_bucket_versioning.html): Sets the versioning state of an existing bucket.

```python
import boto3

s3_client=boto3.client('s3',region_name='us-west-1')
# create a bucket
s3_client.create_bucket(Bucket='wingwing')
# enable versioning
s3_client.put_bucket_versioning(Bucket='wingwing',
                                VersioningConfiguration={'Status': 'Enabled'})
# create folder
s3_client.put_object(Bucket='wingwing',
                     Key='YD/')
```



## Use Boto3 delete objects and bucket

```python
import boto3

# remove S3 objects and buckets
s3_client=boto3.client('s3',region_name='us-west-1')

# grab objects in bucket
response=s3_client.list_objects(Bucket='wingwing')
objects=response['Contents']

# delete objects
for obj in objects:
    s3_client.delete_object(Bucket='wingwing',Key=obj['Key'])

# list s3 objects
# response=s3_client.list_objects(Bucket='neo-external-data')

# delete bucket
s3_client.delete_bucket(Bucket='wingwing')
```





## Use python to read pkl models stored in S3 bucket

```python
import pickle
import boto3

s3=boto3.client('s3')

# Specify the bucket and object key
model_bucket='wingwing'
model_key='final_models/model.pickle'

# Get the object from the bucket
response = s3.get_object(Bucket=model_bucket, Key=model_key)

# get model's data
model_data = response['Body'].read()

# load the model
model=pickle.loads(model_data)
```





## Use python to read s3 parquet file

```python
import boto3
from io import BytesIO

s3=boto3.client('s3')

external_bucket='wingwing'
CPI_key='Ding/data.parquet'

response = s3.get_object(Bucket=external_bucket, Key=CPI_key)

parquet_content = response['Body'].read()

# Load the Parquet content into a Pandas DataFrame
cpi = pd.read_parquet(BytesIO(parquet_content)) # df
```





## Read objects in S3

list_object_v2: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_objects_v2.html (Returns some or all (up to 1,000) of the objects in a bucket with each request)

```python
import boto3

s3_client=boto3.client('s3')

# return objects in a bucket
response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
```

The response format should be:

```json
{
   "ResponseMetadata":{
      "RequestId":"",
      "HostId":"",
      "HTTPStatusCode":200,
      "HTTPHeaders":{
         "x-amz-id-2":"",
         "x-amz-request-id":"",
         "date":"Wed, 07 Jun 2023 12:52:47 GMT",
         "x-amz-bucket-region":"us-west-1",
         "content-type":"application/xml",
         "transfer-encoding":"chunked",
         "server":"AmazonS3"
      },
      "RetryAttempts":0
   },
   "IsTruncated":false,
   "Contents":[
      {
         "Key":"< prefix name + object name>",
         "LastModified":datetime.datetime(2023,6,7,4,43,35),
         "tzinfo=tzlocal())",
         "ETag":"",
         "Size":0,
         "StorageClass":"STANDARD"
      }
   ],
   "Name":"codex-strethfit-test",
   "Prefix":"< prefix name >",
   "MaxKeys":1000,
   "EncodingType":"url",
   "KeyCount":1
}
```

check if the directory is empty:

1. if we create a folder by self in s3, the folder path will occupy a key position, so key count should over than 1 to represent the directory is not empty.

```python
if response['KeyCount']>1:
  	logger.info('The S3 object key is not empty')
else:
  	logger.info('It is empty')
```

2. if the folder under S3 object is created by lambda self, in other words, we do not create a folder as a prefix in advance, the key count should be 0 to represent empty.

```python
if response['KeyCount']=0:
  	logger.info('It is empty')
else:
  	logger.info('It is not empty')
```





## Read object modified time into s3

### If you want to get an object's last modified time in s3. 

head_object: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/head_object.html (retrieves metadata from an object)

```python
s3_client = boto3.client('s3')

response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)

# get the object key
for object in response['Contents']:
  	s3_key = object['Key']
    
    # retrieve the object's metadata
    object_response=s3_client.head_object(Bucket=bucket_name,Key=s3_key)
    last_modified_time=object_response['ResponseMetadata']['HTTPHeaders']['last-modified']
    print('last modified time: ', last_modified_time)
```

The metadata response should be:

```json
{
   "ResponseMetadata":{
      "RequestId":"",
      "HostId":"",
      "HTTPStatusCode":200,
      "HTTPHeaders":{
         "x-amz-id-2":"",
         "x-amz-request-id":"",
         "date":"Tue, 06 Jun 2023 00:19:01 GMT",
         "last-modified":"Fri, 02 Jun 2023 01:49:16 GMT",
         "etag":"",
         "x-amz-server-side-encryption":"",
         "accept-ranges":"",
         "content-type":"",
         "server":"AmazonS3",
         "content-length":"0"
      },
      "RetryAttempts":0
   },
   "AcceptRanges":"bytes",
   "LastModified":datetime.datetime(2023,
   6,
   2,
   1,
   49,
   16,
   "tzinfo=tzutc())",
   "ContentLength":0,
   "",
   "",
   "ServerSideEncryption":"",
   "Metadata":{
      
   }
}
```



### If using boto3 resource

collection.filtering: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/collections.html#filtering ( filter the returned data set)

```python
# Get resources from the default session
s3=boto3.resource('s3')

bucket=s3.Bucket(bucket_name)

for obj in bucket.objects.filter(Prefix=prefix):
    last_modified=obj.last_modified
    print('last_modified_dt: ', last_modified)
```





## Sort s3 objects by write-in time

All(): https://boto3.amazonaws.com/v1/documentation/api/latest/guide/collections.html#when-collections-make-requests 

```python
s3 = boto3.resource('s3')
bucket = s3.Bucket(bucket_name)

objects = list(bucket.objects.filter(Prefix=prefix))
objects.sort(key=lambda obj: obj.last_modified,reverse=True)

for obj in objects:
  last_modified=obj.last_modified
  print('last_modified_dt: ', last_modified)
```

